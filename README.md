# Rethinking Remote Sensing Image Captioning:
From Parameter-Efficient Fine-Tuning to
Knowledge-Driven Training-Free Adapter

**The code and prompts used for LLM-based metrics have been uploaded. Other code and the knowledge base will be updated  soon.**

To illustrate this discrepancy, as illustrated in Fig. 1, we project captions from several widely-used natural and RS
datasets into a shared semantic space using MPNet and perform clustering analysis.  The results reveal stark differences:
natural images exhibit dispersed clusters with diverse semantic relationships due to object diversity and varying perspectives.
In contrast, RSIs captured from fixed aerial/satellite perspectives form compact, overlapping clusters dominated by structured geographic entities.

![dc6d1c912fd1e962a945144490c5aa3](https://github.com/user-attachments/assets/9ca5bf0f-4360-49ba-9726-a4cd35d37ccc)

To demonstrate the accuracy and detail of captions generated by our method, we conducted a qualitative comparison with RS-LLaVA, GeoChat, and VHM on two representative scenes: aircraft and ships.
![caption](https://github.com/user-attachments/assets/0ca1fec9-4a24-4606-ae82-488e89e8abac)
![caption_ship](https://github.com/user-attachments/assets/5365ef5c-d901-4513-b0e3-f5ef3d02af08)
